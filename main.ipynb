{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.models import vgg16\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from Utils.expression_model import ExpressionRecognitionModel\n",
    "import time\n",
    "from yolov7.models.experimental import attempt_load\n",
    "from yolov7.utils.datasets import LoadImages\n",
    "from yolov7.utils.general import check_img_size, non_max_suppression, scale_coords, set_logging\n",
    "from yolov7.utils.torch_utils import select_device, time_synchronized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load emotion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "emotion_model = ExpressionRecognitionModel(num_classes=5)\n",
    "emotion_model.to(device)\n",
    "emotion_model.load_state_dict(torch.load(\"path/to/emotion_model_weights.pth\", map_location=device))\n",
    "\n",
    "def classify_emotion(face_image, emotion_model, device):\n",
    "    emotion_list = ['Cry', 'Surprise', 'angry', 'confuse', 'happy', 'neutral', 'sad']\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    img = transform(face_image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        emotion = emotion_model(img)\n",
    "    emotion = emotion.cpu().numpy()\n",
    "    return emotion_list[np.argmax(emotion)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bounding_boxes(video_path, weights='yolov7.pt', img_size=640, conf_thres=0.25, iou_thres=0.45): # location of yolov7 best weights\n",
    "    set_logging()\n",
    "    device = select_device('')\n",
    "    half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "\n",
    "    model = attempt_load(weights, map_location=device)\n",
    "    stride = int(model.stride.max())\n",
    "    img_size = check_img_size(img_size, s=stride)\n",
    "\n",
    "    if half:\n",
    "        model.half()\n",
    "\n",
    "    dataset = LoadImages(video_path, img_size=img_size, stride=stride)\n",
    "    bounding_boxes = []\n",
    "\n",
    "    t0 = time.time()\n",
    "    for _, img, im0s, _ in dataset:\n",
    "        img = torch.from_numpy(img).to(device)\n",
    "        img = img.half() if half else img.float()\n",
    "        img /= 255.0\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(img, augment=False)[0]\n",
    "\n",
    "        pred = non_max_suppression(pred, conf_thres, iou_thres)\n",
    "\n",
    "        frame_boxes = []\n",
    "        for det in pred:\n",
    "            if len(det):\n",
    "                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0s.shape).round()\n",
    "                frame_boxes.extend([[*xyxy, conf.item(), cls.item()] for *xyxy, conf, cls in reversed(det)])\n",
    "\n",
    "        bounding_boxes.append(frame_boxes)\n",
    "\n",
    "    print(f'Done. ({time.time() - t0:.3f}s)')\n",
    "    return bounding_boxes\n",
    "\n",
    "def process_video_and_classify(video_path, bounding_boxes, emotion_model, emotion_list, output_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file: {video_path}\")\n",
    "        return\n",
    "\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_idx = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_idx < len(bounding_boxes):\n",
    "            boxes = bounding_boxes[frame_idx]\n",
    "\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2, conf, cls = map(int, box)\n",
    "\n",
    "                x1, y1 = max(x1, 0), max(y1, 0)\n",
    "                x2, y2 = min(x2, frame.shape[1]), min(y2, frame.shape[0])\n",
    "\n",
    "                x1, y1 = x1 + int(0.1 * (x2 - x1)), y1 + int(0.1 * (y2 - y1))\n",
    "                x2, y2 = x2 - int(0.1 * (x2 - x1)), y2 - int(0.1 * (y2 - y1))\n",
    "\n",
    "                crop_img = frame[y1:y2, x1:x2]\n",
    "                if crop_img.size == 0:\n",
    "                    continue\n",
    "\n",
    "                crop_img_pil = Image.fromarray(crop_img)\n",
    "\n",
    "                emotion_model.eval()\n",
    "                emotion = classify_emotion(crop_img_pil, emotion_model, device)\n",
    "\n",
    "                label = f'{emotion}'\n",
    "                color = (255, 0, 0)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 1)\n",
    "                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "        out.write(frame)\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f'Output video saved to {output_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"path/to/input/video.mp4\"\n",
    "output_path = \"path/to/output/video.mp4\"\n",
    "emotion_list = ['Cry', 'Surprise', 'angry', 'confuse', 'happy', 'neutral', 'sad']\n",
    "bounding_boxes = extract_bounding_boxes(video_path)\n",
    "process_video_and_classify(video_path, bounding_boxes, emotion_model, emotion_list, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
